tweets = rbind(pos_tweets, neg_tweets, test_tweets)
#build document-term matrix
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,  # we can also removeSparseTerms
stemWords=FALSE)
# train the model
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:10,], as.factor(tweets[1:10,2]) )
# test the validity
predicted = predict(classifier, mat[11:15,]); predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
#In this script we will scrape google queries
# load packages
library(RCurl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
u <- "https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=anthony%20davis%20scouting%20report"
get_google_page_urls(u)
# [1] "http://www.r-project.org/"
# [2] "http://en.wikipedia.org/wiki/R_(programming_language)"
# [3] "http://www.rseek.org/"
# [4] "http://www.gutenberg.org/browse/authors/r"
# [5] "http://sciviews.org/_rgui/"
# [6] "http://www.itc.nl/~rossiter/teach/R/RIntro_ITC.pdf"
# [7] "http://stat.ethz.ch/CRAN/"
# [8] "http://hughesbennett.co.uk/RProject"
# [9] "http://www.warwick.ac.uk/statsdept/user-2011/"
install.packages("XML")
#In this script we will scrape google queries
# load packages
library(RCurl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
u <- "https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=anthony%20davis%20scouting%20report"
get_google_page_urls(u)
# [1] "http://www.r-project.org/"
# [2] "http://en.wikipedia.org/wiki/R_(programming_language)"
# [3] "http://www.rseek.org/"
# [4] "http://www.gutenberg.org/browse/authors/r"
# [5] "http://sciviews.org/_rgui/"
# [6] "http://www.itc.nl/~rossiter/teach/R/RIntro_ITC.pdf"
# [7] "http://stat.ethz.ch/CRAN/"
# [8] "http://hughesbennett.co.uk/RProject"
# [9] "http://www.warwick.ac.uk/statsdept/user-2011/"
#In this script we will scrape google queries
# load packages
library(RCurl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
u <- "http://www.google.co.uk/search?aq=f&gcx=w&sourceid=chrome&ie=UTF-8&q=r+project"
get_google_page_urls(u)
# [1] "http://www.r-project.org/"
# [2] "http://en.wikipedia.org/wiki/R_(programming_language)"
# [3] "http://www.rseek.org/"
# [4] "http://www.gutenberg.org/browse/authors/r"
# [5] "http://sciviews.org/_rgui/"
# [6] "http://www.itc.nl/~rossiter/teach/R/RIntro_ITC.pdf"
# [7] "http://stat.ethz.ch/CRAN/"
# [8] "http://hughesbennett.co.uk/RProject"
# [9] "http://www.warwick.ac.uk/statsdept/user-2011/"
u <- "http://www.google.com/search?aq=f&gcx=w&sourceid=chrome&ie=UTF-8&q=anthony+davis+scouting+report"
get_google_page_urls(u)
links
links
print(links)
links[1]
?getURL
getURL("http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q")
#In this script we will scrape google queries
# source: http://www.r-bloggers.com/web-scraping-google-urls/
# load packages
library(RCurl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
u <- "http://www.google.com/search?aq=f&gcx=w&sourceid=chrome&ie=UTF-8&q=anthony+davis+scouting+report"
get_google_page_urls(u)
text = getURL("http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q")
if(require(XML))
htmlTreeParse(text, asText = TRUE)
# At this point we have the top 9 search terms in links
# We now want to scrape each term in links
?"htmlParse"
?htmltoText
#In this script we will scrape google queries
# source: http://www.r-bloggers.com/web-scraping-google-urls/
# load packages
library(RCurl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
u <- "http://www.google.com/search?aq=f&gcx=w&sourceid=chrome&ie=UTF-8&q=anthony+davis+scouting+report"
get_google_page_urls(u)
# At this point we have the top 9 search terms in links
# We now want to scrape each term in links
htmlToText <- function(input, ...) {
###---PACKAGES ---###
require(RCurl)
require(XML)
###--- LOCAL FUNCTIONS ---###
# Determine how to grab html for a single input element
evaluate_input <- function(input) {
# if input is a .html file
if(file.exists(input)) {
char.vec <- readLines(input, warn = FALSE)
return(paste(char.vec, collapse = ""))
}
# if input is html text
if(grepl("</html>", input, fixed = TRUE)) return(input)
# if input is a URL, probably should use a regex here instead?
if(!grepl(" ", input)) {
# downolad SSL certificate in case of https problem
if(!file.exists("cacert.perm")) download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.perm")
return(getURL(input, followlocation = TRUE, cainfo = "cacert.perm"))
}
# return NULL if none of the conditions above apply
return(NULL)
}
# convert HTML to plain text
convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
# format text vector into one character string
collapse_text <- function(txt) {
return(paste(txt, collapse = " "))
}
###--- MAIN ---###
# STEP 1: Evaluate input
html.list <- lapply(input, evaluate_input)
# STEP 2: Extract text from HTML
text.list <- lapply(html.list, convert_html_to_text)
# STEP 3: Return text
text.vector <- sapply(text.list, collapse_text)
return(text.vector)
}
input <- "http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q"
txt <- htmlToText(input)
txt
?grep
#In this script we will scrape google queries
# source: http://www.r-bloggers.com/web-scraping-google-urls/
# load packages
library(RCurl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
u <- "http://www.google.com/search?aq=f&gcx=w&sourceid=chrome&ie=UTF-8&q=anthony+davis+scouting+report"
get_google_page_urls(u)
# At this point we have the top 9 search terms in links
# We now want to scrape each term in links
htmlToText <- function(input, ...) {
###---PACKAGES ---###
require(RCurl)
require(XML)
###--- LOCAL FUNCTIONS ---###
# Determine how to grab html for a single input element
evaluate_input <- function(input) {
# if input is a .html file
if(file.exists(input)) {
char.vec <- readLines(input, warn = FALSE)
return(paste(char.vec, collapse = ""))
}
# if input is html text
if(grepl("</html>", input, fixed = TRUE)) return(input)
# if input is a URL, probably should use a regex here instead?
if(!grepl(" ", input)) {
# downolad SSL certificate in case of https problem
if(!file.exists("cacert.perm")) download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.perm")
return(getURL(input, followlocation = TRUE, cainfo = "cacert.perm"))
}
# return NULL if none of the conditions above apply
return(NULL)
}
# convert HTML to plain text
convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
# format text vector into one character string
collapse_text <- function(txt) {
return(paste(txt, collapse = " "))
}
###--- MAIN ---###
# STEP 1: Evaluate input
html.list <- lapply(input, evaluate_input)
# STEP 2: Extract text from HTML
text.list <- lapply(html.list, convert_html_to_text)
# STEP 3: Return text
text.vector <- sapply(text.list, collapse_text)
return(text.vector)
}
#input <- "http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q"
#txt <- htmlToText(input)
#txt
## other approach
library(RCurlr)
html <- getURL("http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q")
doc = htmlParse(html, asText=TRUE)
plain.text <- xpathSApply(doc, "//p", xmlValue)
cat(paste(plain.text, collapse = "\n"))
#In this script we will scrape google queries
# source: http://www.r-bloggers.com/web-scraping-google-urls/
# load packages
library(RCurl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
u <- "http://www.google.com/search?aq=f&gcx=w&sourceid=chrome&ie=UTF-8&q=anthony+davis+scouting+report"
get_google_page_urls(u)
# At this point we have the top 9 search terms in links
# We now want to scrape each term in links
htmlToText <- function(input, ...) {
###---PACKAGES ---###
require(RCurl)
require(XML)
###--- LOCAL FUNCTIONS ---###
# Determine how to grab html for a single input element
evaluate_input <- function(input) {
# if input is a .html file
if(file.exists(input)) {
char.vec <- readLines(input, warn = FALSE)
return(paste(char.vec, collapse = ""))
}
# if input is html text
if(grepl("</html>", input, fixed = TRUE)) return(input)
# if input is a URL, probably should use a regex here instead?
if(!grepl(" ", input)) {
# downolad SSL certificate in case of https problem
if(!file.exists("cacert.perm")) download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.perm")
return(getURL(input, followlocation = TRUE, cainfo = "cacert.perm"))
}
# return NULL if none of the conditions above apply
return(NULL)
}
# convert HTML to plain text
convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
# format text vector into one character string
collapse_text <- function(txt) {
return(paste(txt, collapse = " "))
}
###--- MAIN ---###
# STEP 1: Evaluate input
html.list <- lapply(input, evaluate_input)
# STEP 2: Extract text from HTML
text.list <- lapply(html.list, convert_html_to_text)
# STEP 3: Return text
text.vector <- sapply(text.list, collapse_text)
return(text.vector)
}
#input <- "http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q"
#txt <- htmlToText(input)
#txt
## other approach
library(RCurlr)
html <- getURL("http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q")
doc = htmlParse(html, asText=TRUE)
plain.text <- xpathSApply(doc, "//p", xmlValue)
gsub("\\d","", plain.text)
cat(paste(plain.text, collapse = "\n"))
#In this script we will scrape google queries
# source: http://www.r-bloggers.com/web-scraping-google-urls/
# load packages
library(RCurl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
u <- "http://www.google.com/search?aq=f&gcx=w&sourceid=chrome&ie=UTF-8&q=anthony+davis+scouting+report"
get_google_page_urls(u)
# At this point we have the top 9 search terms in links
# We now want to scrape each term in links
htmlToText <- function(input, ...) {
###---PACKAGES ---###
require(RCurl)
require(XML)
###--- LOCAL FUNCTIONS ---###
# Determine how to grab html for a single input element
evaluate_input <- function(input) {
# if input is a .html file
if(file.exists(input)) {
char.vec <- readLines(input, warn = FALSE)
return(paste(char.vec, collapse = ""))
}
# if input is html text
if(grepl("</html>", input, fixed = TRUE)) return(input)
# if input is a URL, probably should use a regex here instead?
if(!grepl(" ", input)) {
# downolad SSL certificate in case of https problem
if(!file.exists("cacert.perm")) download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.perm")
return(getURL(input, followlocation = TRUE, cainfo = "cacert.perm"))
}
# return NULL if none of the conditions above apply
return(NULL)
}
# convert HTML to plain text
convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
# format text vector into one character string
collapse_text <- function(txt) {
return(paste(txt, collapse = " "))
}
###--- MAIN ---###
# STEP 1: Evaluate input
html.list <- lapply(input, evaluate_input)
# STEP 2: Extract text from HTML
text.list <- lapply(html.list, convert_html_to_text)
# STEP 3: Return text
text.vector <- sapply(text.list, collapse_text)
return(text.vector)
}
#input <- "http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q"
#txt <- htmlToText(input)
#txt
## other approach
library(RCurlr)
html <- getURL("http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q")
doc = htmlParse(html, asText=TRUE)
plain.text <- xpathSApply(doc, "//p", xmlValue)
cat(paste(plain.text, collapse = "\n"))
#In this script we will scrape google queries
# source: http://www.r-bloggers.com/web-scraping-google-urls/
# load packages
library(RCurl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
u <- "http://www.google.com/search?aq=f&gcx=w&sourceid=chrome&ie=UTF-8&q=anthony+davis+scouting+report"
get_google_page_urls(u)
# At this point we have the top 9 search terms in links
# We now want to scrape each term in links
htmlToText <- function(input, ...) {
###---PACKAGES ---###
require(RCurl)
require(XML)
###--- LOCAL FUNCTIONS ---###
# Determine how to grab html for a single input element
evaluate_input <- function(input) {
# if input is a .html file
if(file.exists(input)) {
char.vec <- readLines(input, warn = FALSE)
return(paste(char.vec, collapse = ""))
}
# if input is html text
if(grepl("</html>", input, fixed = TRUE)) return(input)
# if input is a URL, probably should use a regex here instead?
if(!grepl(" ", input)) {
# downolad SSL certificate in case of https problem
if(!file.exists("cacert.perm")) download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.perm")
return(getURL(input, followlocation = TRUE, cainfo = "cacert.perm"))
}
# return NULL if none of the conditions above apply
return(NULL)
}
# convert HTML to plain text
convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
# format text vector into one character string
collapse_text <- function(txt) {
return(paste(txt, collapse = " "))
}
###--- MAIN ---###
# STEP 1: Evaluate input
html.list <- lapply(input, evaluate_input)
# STEP 2: Extract text from HTML
text.list <- lapply(html.list, convert_html_to_text)
# STEP 3: Return text
text.vector <- sapply(text.list, collapse_text)
return(text.vector)
}
#input <- "http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q"
#txt <- htmlToText(input)
#txt
## other approach
library(RCurlr)
html <- getURL("http://www.nbadraft.net/players/anthony-davis&sa=U&ved=0CBQQFjAAahUKEwjP-JTBgYvGAhVQCZIKHeDBAJw&usg=AFQjCNEfPGESyW6nrFxLMNanML9Ye0i-5Q")
doc = htmlParse(html, asText=TRUE)
plain.text <- xpathSApply(doc, "//p", xmlValue)
gsub("[0-9]", "", plain.text)
cat(paste(plain.text, collapse = "\n"))
